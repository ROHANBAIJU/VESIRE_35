"""
YOLOv8 TFLite Model Evaluation Script
Generates comprehensive metrics including confusion matrix, precision-recall curves, F1 scores, etc.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, roc_curve, auc
import json
from collections import defaultdict
import cv2
from ultralytics import YOLO

# Configuration
MODEL_PATH = Path('yolo11n.pt')  # Your YOLO model
DATASET_PATH = Path('unified_dataset')  # Your dataset path
LABELS_PATH = Path('api/labels.txt')
OUTPUT_DIR = Path('model_metrics')
OUTPUT_DIR.mkdir(exist_ok=True)

# Load class names
with open(LABELS_PATH, 'r') as f:
    class_names = [line.strip() for line in f.readlines()]

if __name__ == '__main__':
    print(f"üìä Starting model evaluation...")
    print(f"üìÅ Model: {MODEL_PATH}")
    print(f"üìÅ Dataset: {DATASET_PATH}")
    print(f"üìä Classes: {len(class_names)}")

    # Load YOLO model
    print("\nüîÑ Loading model...")
    model = YOLO(str(MODEL_PATH))

# Run validation to get metrics
print("\nüîÑ Running validation on test set...")
results = model.val(
    data=str(DATASET_PATH / 'data.yaml'),
    split='val',  # Use validation split
    conf=0.25,  # Confidence threshold
    iou=0.5,  # IoU threshold for NMS
    save_json=True,  # Save results as JSON
    plots=True,  # Generate plots
    verbose=True
)

print("\n‚úÖ Validation complete!")

# Extract metrics
print("\nüìä Extracting metrics...")

# 1. Overall metrics
print("\n" + "="*60)
print("OVERALL METRICS")
print("="*60)
print(f"mAP50: {results.box.map50:.4f}")
print(f"mAP50-95: {results.box.map:.4f}")
print(f"Precision: {results.box.mp:.4f}")
print(f"Recall: {results.box.mr:.4f}")
print(f"F1 Score: {(2 * results.box.mp * results.box.mr) / (results.box.mp + results.box.mr):.4f}")

# 2. Per-class metrics
print("\n" + "="*60)
print("PER-CLASS METRICS")
print("="*60)
print(f"{'Class':<30} {'Precision':<12} {'Recall':<12} {'mAP50':<12} {'mAP50-95':<12}")
print("-"*78)

# Get per-class metrics
class_metrics = []
for i, class_name in enumerate(class_names):
    if i < len(results.box.ap_class_index):
        idx = results.box.ap_class_index[i]
        precision = results.box.p[i] if i < len(results.box.p) else 0
        recall = results.box.r[i] if i < len(results.box.r) else 0
        ap50 = results.box.ap50[i] if i < len(results.box.ap50) else 0
        ap = results.box.ap[i] if i < len(results.box.ap) else 0
        
        print(f"{class_name:<30} {precision:<12.4f} {recall:<12.4f} {ap50:<12.4f} {ap:<12.4f}")
        
        class_metrics.append({
            'class': class_name,
            'precision': float(precision),
            'recall': float(recall),
            'ap50': float(ap50),
            'ap': float(ap),
            'f1': float(2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0
        })

# 3. Generate Confusion Matrix
print("\nüîÑ Generating confusion matrix...")
# The confusion matrix is automatically generated by YOLO during validation
# It's saved in runs/detect/val/confusion_matrix.png

# 4. Create comprehensive metrics report
print("\nüîÑ Creating comprehensive metrics report...")

# Plot F1 scores
fig, ax = plt.subplots(figsize=(14, 8))
classes = [m['class'] for m in class_metrics]
f1_scores = [m['f1'] for m in class_metrics]
precisions = [m['precision'] for m in class_metrics]
recalls = [m['recall'] for m in class_metrics]

x = np.arange(len(classes))
width = 0.25

bars1 = ax.bar(x - width, f1_scores, width, label='F1 Score', color='#2ecc71')
bars2 = ax.bar(x, precisions, width, label='Precision', color='#3498db')
bars3 = ax.bar(x + width, recalls, width, label='Recall', color='#e74c3c')

ax.set_xlabel('Disease Classes', fontsize=12, fontweight='bold')
ax.set_ylabel('Score', fontsize=12, fontweight='bold')
ax.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(classes, rotation=45, ha='right')
ax.legend()
ax.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.savefig(OUTPUT_DIR / 'per_class_metrics.png', dpi=300, bbox_inches='tight')
print(f"‚úÖ Saved: {OUTPUT_DIR / 'per_class_metrics.png'}")

# Plot mAP comparison
fig, ax = plt.subplots(figsize=(14, 8))
ap50_scores = [m['ap50'] for m in class_metrics]
ap_scores = [m['ap'] for m in class_metrics]

x = np.arange(len(classes))
width = 0.35

bars1 = ax.bar(x - width/2, ap50_scores, width, label='mAP@0.5', color='#9b59b6')
bars2 = ax.bar(x + width/2, ap_scores, width, label='mAP@0.5:0.95', color='#f39c12')

ax.set_xlabel('Disease Classes', fontsize=12, fontweight='bold')
ax.set_ylabel('Average Precision', fontsize=12, fontweight='bold')
ax.set_title('Mean Average Precision (mAP) per Class', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(classes, rotation=45, ha='right')
ax.legend()
ax.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.savefig(OUTPUT_DIR / 'map_comparison.png', dpi=300, bbox_inches='tight')
print(f"‚úÖ Saved: {OUTPUT_DIR / 'map_comparison.png'}")

# Save metrics to JSON
metrics_json = {
    'overall': {
        'mAP50': float(results.box.map50),
        'mAP50-95': float(results.box.map),
        'precision': float(results.box.mp),
        'recall': float(results.box.mr),
        'f1_score': float((2 * results.box.mp * results.box.mr) / (results.box.mp + results.box.mr))
    },
    'per_class': class_metrics,
    'model_info': {
        'model_path': str(MODEL_PATH),
        'num_classes': len(class_names),
        'conf_threshold': 0.25,
        'iou_threshold': 0.5
    }
}

with open(OUTPUT_DIR / 'metrics_report.json', 'w') as f:
    json.dump(metrics_json, f, indent=2)
print(f"‚úÖ Saved: {OUTPUT_DIR / 'metrics_report.json'}")

# Create markdown report
print("\nüîÑ Creating markdown report...")
markdown_report = f"""# AgriScan YOLOv8 Model Performance Report

## Overall Metrics

| Metric | Value |
|--------|-------|
| **mAP@0.5** | {results.box.map50:.4f} |
| **mAP@0.5:0.95** | {results.box.map:.4f} |
| **Precision** | {results.box.mp:.4f} |
| **Recall** | {results.box.mr:.4f} |
| **F1 Score** | {(2 * results.box.mp * results.box.mr) / (results.box.mp + results.box.mr):.4f} |

## Per-Class Performance

| Class | Precision | Recall | F1 Score | mAP@0.5 | mAP@0.5:0.95 |
|-------|-----------|--------|----------|---------|--------------|
"""

for m in class_metrics:
    markdown_report += f"| {m['class']} | {m['precision']:.4f} | {m['recall']:.4f} | {m['f1']:.4f} | {m['ap50']:.4f} | {m['ap']:.4f} |\n"

markdown_report += f"""
## Model Information

- **Model Path**: `{MODEL_PATH}`
- **Number of Classes**: {len(class_names)}
- **Confidence Threshold**: 0.25
- **IoU Threshold**: 0.5
- **Dataset**: `{DATASET_PATH}`

## Visualizations

### Per-Class Metrics
![Per-Class Metrics](per_class_metrics.png)

### mAP Comparison
![mAP Comparison](map_comparison.png)

### Confusion Matrix
The confusion matrix shows the model's prediction accuracy across all classes. It's automatically generated in the YOLO validation results.

## Interpretation

### What These Metrics Mean:

1. **mAP (Mean Average Precision)**:
   - mAP@0.5: Average precision at 50% IoU threshold
   - mAP@0.5:0.95: Average precision across IoU thresholds from 0.5 to 0.95
   - Higher is better (range: 0-1)

2. **Precision**: 
   - Percentage of correct positive predictions
   - How many detected diseases are actually correct
   - Formula: TP / (TP + FP)

3. **Recall**:
   - Percentage of actual diseases detected
   - How many actual diseases the model found
   - Formula: TP / (TP + FN)

4. **F1 Score**:
   - Harmonic mean of precision and recall
   - Balanced measure of model performance
   - Formula: 2 * (Precision * Recall) / (Precision + Recall)

### Performance Analysis:

**Strong Classes** (F1 > 0.8):
"""

strong_classes = [m for m in class_metrics if m['f1'] > 0.8]
for m in sorted(strong_classes, key=lambda x: x['f1'], reverse=True)[:5]:
    markdown_report += f"- **{m['class']}**: F1={m['f1']:.4f}, Precision={m['precision']:.4f}, Recall={m['recall']:.4f}\n"

markdown_report += f"""
**Classes Needing Improvement** (F1 < 0.6):
"""

weak_classes = [m for m in class_metrics if m['f1'] < 0.6]
for m in sorted(weak_classes, key=lambda x: x['f1'])[:5]:
    markdown_report += f"- **{m['class']}**: F1={m['f1']:.4f}, Precision={m['precision']:.4f}, Recall={m['recall']:.4f}\n"

markdown_report += """
## Recommendations for Hackathon Presentation:

1. **Highlight Overall mAP**: Show that your model achieves strong performance across 34 disease classes
2. **Discuss Top Performers**: Emphasize classes with high F1 scores
3. **Address Challenges**: Explain why certain classes are harder to detect (similar visual features, dataset imbalance)
4. **Show Real-world Impact**: Connect metrics to practical agricultural benefits
5. **Present Confusion Matrix**: Visual representation of model's classification accuracy

---
*Generated on {results.save_dir}*
"""

with open(OUTPUT_DIR / 'MODEL_METRICS_REPORT.md', 'w') as f:
    f.write(markdown_report)
print(f"‚úÖ Saved: {OUTPUT_DIR / 'MODEL_METRICS_REPORT.md'}")

print("\n" + "="*60)
print("‚úÖ EVALUATION COMPLETE!")
print("="*60)
print(f"\nüìÅ All metrics saved to: {OUTPUT_DIR}")
print(f"\nüìä Generated files:")
print(f"   - metrics_report.json (JSON data)")
print(f"   - MODEL_METRICS_REPORT.md (Full report)")
print(f"   - per_class_metrics.png (F1/Precision/Recall chart)")
print(f"   - map_comparison.png (mAP comparison chart)")
print(f"\nüìç YOLO also generated:")
print(f"   - Confusion matrix: runs/detect/val/confusion_matrix.png")
print(f"   - PR curves: runs/detect/val/PR_curve.png")
print(f"   - F1 curve: runs/detect/val/F1_curve.png")
print(f"   - Results: runs/detect/val/results.png")
print("\nüéâ Use these metrics in your hackathon presentation!")
