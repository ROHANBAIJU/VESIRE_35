"""
YOLOv8 TFLite Model Evaluation Script
Generates comprehensive metrics including confusion matrix, precision-recall curves, F1 scores, etc.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, roc_curve, auc
import json
from collections import defaultdict
import cv2
from ultralytics import YOLO

# Configuration
MODEL_PATH = Path('yolo11n.pt')  # Your YOLO model
DATASET_PATH = Path('unified_dataset')  # Your dataset path
LABELS_PATH = Path('api/labels.txt')
OUTPUT_DIR = Path('model_metrics')
OUTPUT_DIR.mkdir(exist_ok=True)

# Load class names
with open(LABELS_PATH, 'r') as f:
    class_names = [line.strip() for line in f.readlines()]

if __name__ == '__main__':
    print(f"üìä Starting model evaluation...")
    print(f"üìÅ Model: {MODEL_PATH}")
    print(f"üìÅ Dataset: {DATASET_PATH}")
    print(f"üìä Classes: {len(class_names)}")

    # Load YOLO model
    print("\nüîÑ Loading model...")
    model = YOLO(str(MODEL_PATH))

    # Run validation to get metrics
    print("\nüîÑ Running validation on test set...")
    results = model.val(
        data=str(DATASET_PATH / 'data.yaml'),
        split='val',  # Use validation split
        conf=0.25,  # Confidence threshold
        iou=0.5,  # IoU threshold for NMS
        save_json=True,  # Save results as JSON
        plots=True,  # Generate plots
        verbose=True
    )

    print("\n‚úÖ Validation complete!")

    # Extract metrics
    print("\nüìä Extracting metrics...")

    # 1. Overall metrics
    print("\n" + "="*60)
    print("OVERALL METRICS")
    print("="*60)
    print(f"mAP50: {results.box.map50:.4f}")
    print(f"mAP50-95: {results.box.map:.4f}")
    print(f"Precision: {results.box.mp:.4f}")
    print(f"Recall: {results.box.mr:.4f}")
    print(f"F1 Score: {(2 * results.box.mp * results.box.mr) / (results.box.mp + results.box.mr):.4f}")

    # 2. Per-class metrics
    print("\n" + "="*60)
    print("PER-CLASS METRICS")
    print("="*60)
    print(f"{'Class':<30} {'Precision':<12} {'Recall':<12} {'mAP50':<12} {'mAP50-95':<12}")
    print("-"*78)

    # Get per-class metrics
    class_metrics = []
    for i, class_name in enumerate(class_names):
        if i < len(results.box.ap_class_index):
            idx = results.box.ap_class_index[i]
            precision = results.box.p[i] if i < len(results.box.p) else 0
            recall = results.box.r[i] if i < len(results.box.r) else 0
            ap50 = results.box.ap50[i] if i < len(results.box.ap50) else 0
            ap = results.box.ap[i] if i < len(results.box.ap) else 0
            
            print(f"{class_name:<30} {precision:<12.4f} {recall:<12.4f} {ap50:<12.4f} {ap:<12.4f}")
            
            class_metrics.append({
                'class': class_name,
                'precision': float(precision),
                'recall': float(recall),
                'ap50': float(ap50),
                'ap': float(ap),
                'f1': float(2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0
            })

    # 3. Generate Confusion Matrix
    print("\nüîÑ Generating confusion matrix...")
    # The confusion matrix is automatically generated by YOLO during validation
    # It's saved in runs/detect/val/confusion_matrix.png

    # 4. Create comprehensive metrics report
    print("\nüîÑ Creating comprehensive metrics report...")

    # Plot F1 scores
    fig, ax = plt.subplots(figsize=(14, 8))
    classes = [m['class'] for m in class_metrics]
    f1_scores = [m['f1'] for m in class_metrics]
    precisions = [m['precision'] for m in class_metrics]
    recalls = [m['recall'] for m in class_metrics]

    x = np.arange(len(classes))
    width = 0.25

    bars1 = ax.bar(x - width, f1_scores, width, label='F1 Score', color='#2ecc71')
    bars2 = ax.bar(x, precisions, width, label='Precision', color='#3498db')
    bars3 = ax.bar(x + width, recalls, width, label='Recall', color='#e74c3c')

    ax.set_xlabel('Disease Classes', fontsize=12, fontweight='bold')
    ax.set_ylabel('Score', fontsize=12, fontweight='bold')
    ax.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(classes, rotation=45, ha='right')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'per_class_metrics.png', dpi=300, bbox_inches='tight')
    print(f"‚úÖ Saved: {OUTPUT_DIR / 'per_class_metrics.png'}")

    # Plot mAP comparison
    fig, ax = plt.subplots(figsize=(14, 8))
    ap50_scores = [m['ap50'] for m in class_metrics]
    ap_scores = [m['ap'] for m in class_metrics]

    x = np.arange(len(classes))
    width = 0.35

    bars1 = ax.bar(x - width/2, ap50_scores, width, label='mAP@0.5', color='#9b59b6')
    bars2 = ax.bar(x + width/2, ap_scores, width, label='mAP@0.5:0.95', color='#e67e22')

    ax.set_xlabel('Disease Classes', fontsize=12, fontweight='bold')
    ax.set_ylabel('mAP Score', fontsize=12, fontweight='bold')
    ax.set_title('Mean Average Precision Comparison', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(classes, rotation=45, ha='right')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'map_comparison.png', dpi=300, bbox_inches='tight')
    print(f"‚úÖ Saved: {OUTPUT_DIR / 'map_comparison.png'}")

    # 5. Save JSON report
    report = {
        'overall_metrics': {
            'mAP50': float(results.box.map50),
            'mAP50_95': float(results.box.map),
            'precision': float(results.box.mp),
            'recall': float(results.box.mr),
            'f1_score': float((2 * results.box.mp * results.box.mr) / (results.box.mp + results.box.mr))
        },
        'per_class_metrics': class_metrics,
        'config': {
            'model': str(MODEL_PATH),
            'dataset': str(DATASET_PATH),
            'num_classes': len(class_names),
            'confidence_threshold': 0.25,
            'iou_threshold': 0.5
        }
    }

    with open(OUTPUT_DIR / 'metrics_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    print(f"‚úÖ Saved: {OUTPUT_DIR / 'metrics_report.json'}")

    # 6. Generate Markdown Report
    markdown_report = f"""# Model Evaluation Report

## Overall Performance

| Metric | Score |
|--------|-------|
| **mAP@0.5** | {results.box.map50:.4f} |
| **mAP@0.5:0.95** | {results.box.map:.4f} |
| **Precision** | {results.box.mp:.4f} |
| **Recall** | {results.box.mr:.4f} |
| **F1 Score** | {(2 * results.box.mp * results.box.mr) / (results.box.mp + results.box.mr):.4f} |

## Per-Class Metrics

| Class | Precision | Recall | F1 | mAP@0.5 | mAP@0.5:0.95 |
|-------|-----------|--------|-----|---------|--------------|
"""
    
    for m in class_metrics:
        markdown_report += f"| {m['class']} | {m['precision']:.4f} | {m['recall']:.4f} | {m['f1']:.4f} | {m['ap50']:.4f} | {m['ap']:.4f} |\n"

    markdown_report += f"""
## Interpretation

### What These Metrics Mean:

- **mAP (Mean Average Precision)**: Average precision across all classes. Higher is better.
  - **mAP@0.5**: Accuracy at 50% IoU threshold. Your score: {results.box.map50:.1%}
  - **mAP@0.5:0.95**: Average across multiple IoU thresholds (0.5 to 0.95). Your score: {results.box.map:.1%}

- **Precision**: {results.box.mp:.1%} of detections are correct (low false positives)
- **Recall**: {results.box.mr:.1%} of actual diseases are detected (low false negatives)
- **F1 Score**: Balanced measure combining precision and recall

### Top Performing Classes:
"""
    
    # Sort by F1 score
    top_classes = sorted(class_metrics, key=lambda x: x['f1'], reverse=True)[:5]
    for i, m in enumerate(top_classes, 1):
        markdown_report += f"{i}. **{m['class']}**: F1={m['f1']:.4f}, Precision={m['precision']:.4f}, Recall={m['recall']:.4f}\n"

    markdown_report += """
### Classes Needing Improvement:
"""
    
    # Bottom 5 by F1 score
    bottom_classes = sorted(class_metrics, key=lambda x: x['f1'])[:5]
    for i, m in enumerate(bottom_classes, 1):
        markdown_report += f"{i}. **{m['class']}**: F1={m['f1']:.4f}, Precision={m['precision']:.4f}, Recall={m['recall']:.4f}\n"

    markdown_report += """
## Recommendations for Hackathon Presentation:

1. **Highlight Overall mAP**: Show that your model achieves strong performance across 34 disease classes
2. **Discuss Top Performers**: Emphasize classes with high F1 scores
3. **Address Challenges**: Explain why certain classes are harder to detect (similar visual features, dataset imbalance)
4. **Show Real-world Impact**: Connect metrics to practical agricultural benefits
5. **Present Confusion Matrix**: Visual representation of model's classification accuracy

---
*Generated from validation results*
"""

    with open(OUTPUT_DIR / 'MODEL_METRICS_REPORT.md', 'w') as f:
        f.write(markdown_report)
    print(f"‚úÖ Saved: {OUTPUT_DIR / 'MODEL_METRICS_REPORT.md'}")

    print("\n" + "="*60)
    print("‚úÖ EVALUATION COMPLETE!")
    print("="*60)
    print(f"\nüìÅ All metrics saved to: {OUTPUT_DIR}")
    print(f"\nüìä Generated files:")
    print(f"   - metrics_report.json (JSON data)")
    print(f"   - MODEL_METRICS_REPORT.md (Full report)")
    print(f"   - per_class_metrics.png (F1/Precision/Recall chart)")
    print(f"   - map_comparison.png (mAP comparison chart)")
    print(f"\nüìç YOLO also generated:")
    print(f"   - Confusion matrix: runs/detect/val/confusion_matrix.png")
    print(f"   - PR curves: runs/detect/val/PR_curve.png")
    print(f"   - F1 curve: runs/detect/val/F1_curve.png")
    print(f"   - Results: runs/detect/val/results.png")
    print("\nüéâ Use these metrics in your hackathon presentation!")
